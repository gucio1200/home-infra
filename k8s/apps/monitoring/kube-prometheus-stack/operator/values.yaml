---
# Default values
# https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
defaultRules:
  rules:
    kubeApiserverAvailability: true
    kubeApiserver: true

kubeApiServer:
  enabled: true
  serviceMonitor:
    metricRelabelings:
      # Remove duplicate metrics
      - sourceLabels: ["__name__"]
        regex: "(aggregator_openapi|aggregator_unavailable|apiextensions_openapi|apiserver_admission|apiserver_audit|apiserver_cache|apiserver_cel|apiserver_client|apiserver_crd|apiserver_current|apiserver_envelope|apiserver_flowcontrol|apiserver_init|apiserver_kube|apiserver_longrunning|apiserver_request|apiserver_requested|apiserver_response|apiserver_selfrequest|apiserver_storage|apiserver_terminated|apiserver_tls|apiserver_watch|apiserver_webhooks|authenticated_user|authentication|disabled_metric|etcd_bookmark|etcd_lease|etcd_request|field_validation|get_token|go|grpc_client|hidden_metric|kube_apiserver|kubernetes_build|kubernetes_feature|node_authorizer|pod_security|process_cpu|process_max|process_open|process_resident|process_start|process_virtual|registered_metric|rest_client|scrape_duration|scrape_samples|scrape_series|serviceaccount_legacy|serviceaccount_stale|serviceaccount_valid|watch_cache|workqueue)_(.+)"
        action: keep
      - sourceLabels: ["__name__"]
        regex: (apiserver|etcd|rest_client)_request(|_sli|_slo)_duration_seconds_bucket
        action: drop
      - sourceLabels: ["__name__"]
        regex: (apiserver_response_sizes_bucket|apiserver_watch_events_sizes_bucket)
        action: drop

kubeControllerManager:
  enabled: false

kubeEtcd:
  enabled: false

kubeProxy:
  enabled: false

kubelet:
  enabled: true
  serviceMonitor:
    metricRelabelings:
      - sourceLabels: ["__name__"]
        regex: "(apiserver_audit|apiserver_client|apiserver_delegated|apiserver_envelope|apiserver_storage|apiserver_webhooks|authentication_token|cadvisor_version|container_blkio|container_cpu|container_fs|container_last|container_memory|container_network|container_oom|container_processes|container|csi_operations|disabled_metric|get_token|go|hidden_metric|kubelet_certificate|kubelet_cgroup|kubelet_container|kubelet_containers|kubelet_cpu|kubelet_device|kubelet_graceful|kubelet_http|kubelet_lifecycle|kubelet_managed|kubelet_node|kubelet_pleg|kubelet_pod|kubelet_run|kubelet_running|kubelet_runtime|kubelet_server|kubelet_started|kubelet_volume|kubernetes_build|kubernetes_feature|machine_cpu|machine_memory|machine_nvm|machine_scrape|node_namespace|plugin_manager|prober_probe|process_cpu|process_max|process_open|process_resident|process_start|process_virtual|registered_metric|rest_client|scrape_duration|scrape_samples|scrape_series|storage_operation|volume_manager|volume_operation|workqueue)_(.+)"
        action: keep
      - sourceLabels: ["node"]
        targetLabel: instance
        action: replace

kubeScheduler:
  enabled: false

kube-state-metrics:
  enabled: true
  metricLabelsAllowlist:
    - "persistentvolumeclaims=[*]"
  prometheus:
    monitor:
      enabled: true
      relabelings:
        - action: replace
          regex: (.*)
          replacement: $1
          sourceLabels: ["__meta_kubernetes_pod_node_name"]
          targetLabel: kubernetes_node

nodeExporter:
      enabled: true

prometheusOperator:
  createCustomResource: true
  configReloaderCpu: 200m
  resources:
    requests:
      memory: 164Mi
  admissionWebhooks:
    enabled: true

alertmanager:
  alertmanagerSpec:
    podAntiAffinity: "hard"
  config:
    global:
      resolve_timeout: 5m
    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      # Apply inhibition if the alertname is the same.
      equal: ['alertname', 'namespace']
    templates: ["*.tmpl"]
  templateFiles:
    pagerduty-custom.tmpl: |-
      {{- define "pagerduty.custom.description" -}}[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if ne .CommonAnnotations.summary ""}}{{ .CommonAnnotations.summary }} {{ else if ne .CommonAnnotations.message ""}}{{ .CommonAnnotations.message }} {{ else if ne .CommonAnnotations.description ""}}{{ .CommonAnnotations.description }} {{ else }}{{ .CommonLabels.alertname }}{{ end }}{{- end -}}
  storage:
    volumeClaimTemplate:
      spec:
        resources:
          requests:
            storage: 1Gi
  ingress:
    enabled: true
    pathType: Prefix
    ingressClassName: nginx
    annotations:
      hajimari.io/enable: "true"
      nginx.ingress.kubernetes.io/whitelist-source-range: |
        10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
      hajimari.io/appName: "Alert Manager"
      hajimari.io/icon: mdi:alert-decagram-outline
    hosts:
      - &host alert-manager.k8sj.io
    tls:
      - hosts:
          - *host

prometheus:
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      hajimari.io/enable: "true"
      nginx.ingress.kubernetes.io/whitelist-source-range: |
        10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
      hajimari.io/appName: Prometheus
      hajimari.io/icon: simple-icons:prometheus
    pathType: Prefix
    hosts:
      - &host prometheus.k8sj.io
    tls:
      - hosts:
          - *host

  prometheusSpec:
    additionalScrapeConfigs:
    
    - job_name: minio-job
      metrics_path: /minio/v2/metrics/cluster
      scheme: http
      static_configs:
        - targets: [s3.dom.lan]
        
#    - job_name: openwrt
#      static_configs:
#        - targets: [192.168.2.5:9100,192.168.2.6:9100,192.168.2.2:9100]
        
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name

    - job_name: "linkerd-controller"
      kubernetes_sd_configs:
        - role: pod
          namespaces:
            names:
              - linkerd
      relabel_configs:
        - source_labels:
            - __meta_kubernetes_pod_container_port_name
          action: keep
          regex: admin-http
        - source_labels: [__meta_kubernetes_pod_container_name]
          action: replace
          target_label: component

    - job_name: "linkerd-service-mirror"
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        - source_labels:
            - __meta_kubernetes_pod_label_linkerd_io_control_plane_component
            - __meta_kubernetes_pod_container_port_name
          action: keep
          regex: linkerd-service-mirror;admin-http$
        - source_labels: [__meta_kubernetes_pod_container_name]
          action: replace
          target_label: component

    - job_name: "linkerd-proxy"
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        - source_labels:
            - __meta_kubernetes_pod_container_name
            - __meta_kubernetes_pod_container_port_name
            - __meta_kubernetes_pod_label_linkerd_io_control_plane_ns
          action: keep
          regex: ^linkerd-proxy;linkerd-admin;linkerd$
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: pod
        # special case k8s' "job" label, to not interfere with prometheus' "job"
        # label
        # __meta_kubernetes_pod_label_linkerd_io_proxy_job=foo =>
        # k8s_job=foo
        - source_labels: [__meta_kubernetes_pod_label_linkerd_io_proxy_job]
          action: replace
          target_label: k8s_job
        # drop __meta_kubernetes_pod_label_linkerd_io_proxy_job
        - action: labeldrop
          regex: __meta_kubernetes_pod_label_linkerd_io_proxy_job
        # __meta_kubernetes_pod_label_linkerd_io_proxy_deployment=foo =>
        # deployment=foo
        - action: labelmap
          regex: __meta_kubernetes_pod_label_linkerd_io_proxy_(.+)
        # drop all labels that we just made copies of in the previous labelmap
        - action: labeldrop
          regex: __meta_kubernetes_pod_label_linkerd_io_proxy_(.+)
        # __meta_kubernetes_pod_label_linkerd_io_foo=bar =>
        # foo=bar
        - action: labelmap
          regex: __meta_kubernetes_pod_label_linkerd_io_(.+)
        # Copy all pod labels to tmp labels
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
          replacement: __tmp_pod_label_$1
        # Take `linkerd_io_` prefixed labels and copy them without the prefix
        - action: labelmap
          regex: __tmp_pod_label_linkerd_io_(.+)
          replacement: __tmp_pod_label_$1
        # Drop the `linkerd_io_` originals
        - action: labeldrop
          regex: __tmp_pod_label_linkerd_io_(.+)
        # Copy tmp labels into real labels
        - action: labelmap
          regex: __tmp_pod_label_(.+)

    replicas: 1
    replicaExternalLabelName: "__replica__"
    externalLabels:
      cluster: k8s
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false
    retention: 2d
    retentionSize: "6GB"
    enableAdminAPI: true
    walCompression: true
    resources:
      requests:
        memory: 1500Mi
      limits:
        memory: 2500Mi
    storageSpec:
      volumeClaimTemplate:
        spec:
          resources:
            requests:
              storage: 20Gi

    thanos:
      image: quay.io/thanos/thanos:v0.34.1@sha256:567346c3f6ff2927c2c6c0daad977b2213f62d45eca54d48afd19e6deb902181
      objectStorageConfig:
        name: thanos-objstore-secret
        key: objstore.yml
      # renovate: datasource=docker depName=quay.io/thanos/thanos
      version: "0.34.0"

  thanosService:
    enabled: true

  thanosServiceMonitor:
    enabled: true

grafana:
  enabled: false
  forceDeployDashboards: true
  sidecar:
    dashboards:
      multicluster:
        etcd:
          enabled: true


